{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQjA4hV+bvm7XDEULozzMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wpradamancilla/prueba/blob/master/Desafio%204\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamiento de lenguaje natural\n",
        "\n",
        "LSTM Bot QA\n",
        "Datos\n",
        "\n",
        "El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA)."
      ],
      "metadata": {
        "id": "kF9Vv7Fv5s-j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNn8kzuT5rWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ],
      "metadata": {
        "id": "v8xyYNJX5ns1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, LSTM, SimpleRNN, Input, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "CtAopimO5xnD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDYhCAIa56ix",
        "outputId": "6abbcd10-0be3-41c3-b37b-2b8f058f4a18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download\n",
            "To: /content/data_volunteers.json\n",
            "100%|██████████| 2.58M/2.58M [00:00<00:00, 179MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_file\n",
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data será un diccionario\n",
        "\n"
      ],
      "metadata": {
        "id": "9RV1ZReS6BlL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmpPyC-P6FDe",
        "outputId": "dca113c8-b7a0-443e-a57c-972cddd7c14b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene\n",
        "        output_sentence = output + ' '\n",
        "        # output sentence input (decoder_input) tiene\n",
        "        output_sentence_input = ' ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpLvyKtS6J39",
        "outputId": "86ebcb07-2ec5-4fc1-af4f-d729c014572f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLg-Zw8D6Nlo",
        "outputId": "16ea64f5-6629-4e60-968f-af6530d74a7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  ', ' not bad and you ')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Preprocesamiento\n",
        "\n",
        "Realizar el preprocesamiento necesario para obtener:\n",
        "\n",
        "word2idx_inputs, max_input_len\n",
        "word2idx_outputs, max_out_len, num_words_output\n",
        "encoder_input_sequences, decoder_output_sequences, decoder_targets\n",
        "3 - Preparar los embeddings\n",
        "\n",
        "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores\n",
        "\n",
        "4 - Entrenar el modelo\n",
        "\n",
        "Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase.\n",
        "\n",
        "5 - Inferencia\n",
        "\n",
        "Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."
      ],
      "metadata": {
        "id": "I9BsRJ246SSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet\n",
        "!pip install tensorflow scikit-learn --quiet\n"
      ],
      "metadata": {
        "id": "h-F8dWXy6Rte"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "# Descargar el archivo GloVe desde la URL\n",
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "output_zip = 'glove.6B.zip'\n",
        "\n",
        "if not os.path.exists(output_zip):\n",
        "    gdown.download(glove_url, output_zip, quiet=False)\n",
        "\n",
        "# Descomprimir el archivo\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "\n",
        "print(\"Archivo GloVe descargado y descomprimido.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifmXxDny6q5g",
        "outputId": "29fc8fd3-8fd7-4b81-a33e-1048babe975d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: http://nlp.stanford.edu/data/glove.6B.zip\n",
            "To: /content/glove.6B.zip\n",
            "100%|██████████| 862M/862M [02:43<00:00, 5.27MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo GloVe descargado y descomprimido.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Inicializar el tokenizador para las secuencias de entrada y salida\n",
        "tokenizer_inputs = Tokenizer()\n",
        "tokenizer_outputs = Tokenizer()\n",
        "\n",
        "# Agregar los tokens especiales a las oraciones de salida\n",
        "output_sentences = ['<start> ' + sentence + ' <end>' for sentence in output_sentences]\n",
        "output_sentences_inputs = ['<start> ' + sentence for sentence in output_sentences_inputs]\n",
        "\n",
        "# Crear el diccionario idx2word_outputs para mapear índices a palabras\n",
        "idx2word_outputs = {idx: word for word, idx in word2idx_outputs.items()}\n",
        "\n",
        "# Ajustar el tokenizador con los textos\n",
        "tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "tokenizer_outputs.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "\n",
        "# Diccionarios de palabra a índice\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "\n",
        "# Longitudes máximas de las secuencias\n",
        "max_input_len = max([len(x.split()) for x in input_sentences])\n",
        "max_out_len = max([len(x.split()) for x in output_sentences])\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "# Convertir las oraciones en secuencias numéricas\n",
        "encoder_input_sequences = tokenizer_inputs.texts_to_sequences(input_sentences)\n",
        "decoder_output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "decoder_output_input_sequences = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "# Aplicar padding para que todas las secuencias tengan la misma longitud\n",
        "encoder_input_sequences = pad_sequences(encoder_input_sequences, maxlen=max_input_len)\n",
        "decoder_output_sequences = pad_sequences(decoder_output_sequences, maxlen=max_out_len)\n",
        "decoder_output_input_sequences = pad_sequences(decoder_output_input_sequences, maxlen=max_out_len)\n",
        "\n",
        "# Crear la matriz de objetivos del decodificador con codificación one-hot\n",
        "decoder_targets = np.zeros((len(input_sentences), max_out_len, num_words_output), dtype='float32')\n",
        "\n",
        "for i, seq in enumerate(decoder_output_sequences):\n",
        "    for t, word in enumerate(seq):\n",
        "        if t > 0:\n",
        "            decoder_targets[i, t - 1, word] = 1\n",
        "\n",
        "print(\"Preprocesamiento completado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_SxQLAN6zkX",
        "outputId": "4f0d6a5b-702d-40c6-a8f6-87f242589df3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocesamiento completado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los embeddings de GloVe desde el archivo descomprimido\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Crear la matriz de embeddings para las palabras de entrada\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word2idx_inputs) + 1, embedding_dim))\n",
        "\n",
        "for word, idx in word2idx_inputs.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "print(\"Matriz de embeddings creada.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fwzana564e8",
        "outputId": "6c409bd4-233b-4821-c2f7-8451d8076ebb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de embeddings creada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "encoder_embedding = Embedding(input_dim=len(word2idx_inputs) + 1,\n",
        "                              output_dim=embedding_dim,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=max_input_len,\n",
        "                              trainable=False)(encoder_inputs)\n",
        "\n",
        "encoder_lstm, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_out_len,))\n",
        "decoder_embedding = Embedding(input_dim=num_words_output, output_dim=embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Modelo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit([encoder_input_sequences, decoder_output_input_sequences],\n",
        "          decoder_targets,\n",
        "          batch_size=64,\n",
        "          epochs=20,\n",
        "          validation_split=0.2)\n",
        "\n",
        "print(\"Entrenamiento completado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiCo8NK_8GJX",
        "outputId": "5c4b2b5d-0d9e-4036-fb26-d6a8bd627dbc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 389ms/step - accuracy: 0.3781 - loss: 4.0928 - val_accuracy: 0.5068 - val_loss: 2.1372\n",
            "Epoch 2/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 383ms/step - accuracy: 0.4966 - loss: 2.0464 - val_accuracy: 0.5316 - val_loss: 1.9922\n",
            "Epoch 3/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 412ms/step - accuracy: 0.5262 - loss: 1.8889 - val_accuracy: 0.5440 - val_loss: 1.9088\n",
            "Epoch 4/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 453ms/step - accuracy: 0.5470 - loss: 1.7474 - val_accuracy: 0.5387 - val_loss: 1.8542\n",
            "Epoch 5/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 421ms/step - accuracy: 0.5477 - loss: 1.6886 - val_accuracy: 0.5434 - val_loss: 1.8371\n",
            "Epoch 6/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 410ms/step - accuracy: 0.5559 - loss: 1.6216 - val_accuracy: 0.5444 - val_loss: 1.8011\n",
            "Epoch 7/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 384ms/step - accuracy: 0.5595 - loss: 1.5809 - val_accuracy: 0.5495 - val_loss: 1.7812\n",
            "Epoch 8/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 398ms/step - accuracy: 0.5644 - loss: 1.5422 - val_accuracy: 0.5488 - val_loss: 1.7746\n",
            "Epoch 9/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 431ms/step - accuracy: 0.5731 - loss: 1.5038 - val_accuracy: 0.5524 - val_loss: 1.7679\n",
            "Epoch 10/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 389ms/step - accuracy: 0.5723 - loss: 1.4906 - val_accuracy: 0.5564 - val_loss: 1.7683\n",
            "Epoch 11/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 415ms/step - accuracy: 0.5739 - loss: 1.4712 - val_accuracy: 0.5522 - val_loss: 1.7777\n",
            "Epoch 12/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 384ms/step - accuracy: 0.5741 - loss: 1.4611 - val_accuracy: 0.5531 - val_loss: 1.7805\n",
            "Epoch 13/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 393ms/step - accuracy: 0.5769 - loss: 1.4392 - val_accuracy: 0.5568 - val_loss: 1.7832\n",
            "Epoch 14/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 453ms/step - accuracy: 0.5823 - loss: 1.4114 - val_accuracy: 0.5593 - val_loss: 1.7777\n",
            "Epoch 15/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 387ms/step - accuracy: 0.5844 - loss: 1.3927 - val_accuracy: 0.5610 - val_loss: 1.7871\n",
            "Epoch 16/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 383ms/step - accuracy: 0.5900 - loss: 1.3901 - val_accuracy: 0.5568 - val_loss: 1.8006\n",
            "Epoch 17/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 399ms/step - accuracy: 0.6032 - loss: 1.3874 - val_accuracy: 0.6353 - val_loss: 1.7969\n",
            "Epoch 18/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 455ms/step - accuracy: 0.6449 - loss: 1.3527 - val_accuracy: 0.6358 - val_loss: 1.7985\n",
            "Epoch 19/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 381ms/step - accuracy: 0.6388 - loss: 1.3642 - val_accuracy: 0.6345 - val_loss: 1.8164\n",
            "Epoch 20/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 398ms/step - accuracy: 0.6603 - loss: 1.3584 - val_accuracy: 0.5676 - val_loss: 1.8300\n",
            "Entrenamiento completado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La pérdida se redujo de 4 a 1.3 con el paso de las epocas, lo que indica que el modelo está aprendiendo a generar secuencias que se asemejan a los datos de entrenamiento. Por otro lado, la precisión alcanzó un nivel aceptable (desde el 38% hasta el 66% al final del entrenamiento), lo cual sugiere que el modelo está capturando patrones básicos en las secuencias de entrada y salida. La diferencia entre la pérdida de entrenamiento y la pérdida de validación (gap) se mantuvo dentro de un rango razonable, lo que indica que el modelo no está sobreajustado ni subajustado."
      ],
      "metadata": {
        "id": "fTFbzfb2Jyuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n"
      ],
      "metadata": {
        "id": "jnwaStMZ98Nc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Definir las entradas de estado para el modelo de decoder\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Nueva instancia de la capa de embedding para el modelo de inferencia\n",
        "decoder_embedding_inference = Embedding(input_dim=num_words_output, output_dim=embedding_dim)\n",
        "\n",
        "# Aplicar la nueva capa de embedding a las entradas del decoder\n",
        "decoder_embedding2 = decoder_embedding_inference(decoder_inputs)\n",
        "\n",
        "# LSTM del decoder para inferencia\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedding2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Modelo del decoder para inferencia\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n"
      ],
      "metadata": {
        "id": "hgjm5bj6A-yQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oh4Rm4IGBU8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs.get('<start>', 0)\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = idx2word_outputs.get(sampled_token_index, '')\n",
        "\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_out_len:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Probar con una secuencia de entrada\n",
        "input_seq = encoder_input_sequences[0:1]\n",
        "print(\"Input:\", input_sentences[0])\n",
        "print(\"Decoded:\", decode_sequence(input_seq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8RbvwBlFiLp",
        "outputId": "9cbb59f8-4ca6-4975-f763-7bfe04533d3b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: hello \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Decoded:       i how how are you end end end end start start start start start start start\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo logra generar secuencias completas, lo que indica que la inferencia se está realizando de manera adecuada. Sin embargo, al identificar i how how are you end end end end start start start start start start start, estas respuestas tienen problemas de repetición de palabras y tokens especiales (start, end), lo que sugiere que el modelo podría no estar capturando bien el contexto de las secuencias de entrada.\n",
        "La generación de secuencias repetitivas o irrelevantes puede ser el resultado de un vocabulario limitado, un entrenamiento insuficiente, o una falta de mecanismos de atención para mejorar la coherencia.\n",
        "\n",
        "Como opciones de mejora se puede proponer mejorar la lógica de parada en la función de decodificación, ajustar la temperatura de muestreo, e implementar beam search."
      ],
      "metadata": {
        "id": "THVbPI29KbQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5g1B29jIOOpe"
      }
    }
  ]
}